function model_output = model_SM_RL_all_choices(params, actions, rewards, mdp, sim)
% hi toru
% note that mu2 == right bandit ==  c=2 == free choice = 1

    dbstop if error;
    G = mdp.G; % num of games

    associability_weight = params.associability_weight;
    initial_associability = params.initial_associability;
    side_bias = params.side_bias;
    noise_learning_rate = params.noise_learning_rate;
    baseline_info_bonus = params.baseline_info_bonus;
    baseline_noise = params.baseline_noise;
    initial_mu = params.initial_mu;
    learning_rate = params.learning_rate;
    reward_sensitivity = params.reward_sensitivity;

    
    % indicate if want one parameter to control DE/RE or keep separate
    if mdp.combined_DE_RE_horizon
        DE_RE_horizon = params.DE_RE_horizon;
    else
        info_bonus = params.info_bonus;
        random_exp = params.random_exp;
    end
    
    %%% FIT BEHAVIOR
    action_probs = nan(G,9);
    pred_errors = nan(G,10);
    pred_errors_alpha = nan(G,9);
    exp_vals = nan(G,10);
    alpha = nan(G,10);
    for g=1:G  % loop over games
        % horizon is 1
        if mdp.C1(g)==1
            T = 1;
            Y = 1;
        else
            % horizon is 5
            if mdp.combined_DE_RE_horizon
                T = 1+DE_RE_horizon;
                Y = 1+DE_RE_horizon;
            else
                T = 1+info_bonus;
                Y = 1+random_exp;                    
            end
        end
        mu1 = [initial_mu nan nan nan nan nan nan nan nan];
        mu2 = [initial_mu nan nan nan nan nan nan nan nan];
        noise = [baseline_noise * Y nan nan nan nan nan nan nan nan]; 
        associability1 = [initial_associability nan nan nan nan nan nan nan nan];
        associability2 = [initial_associability nan nan nan nan nan nan nan nan];

        num_choices = sum(~isnan(actions(g,:)));

        for t=1:num_choices  % loop over forced-choice trials

            if t >= 5

               
               %decision = 1/(1+exp(Q1-Q2)/noise))
                deltaR = mu1(t) - mu2(t);
                deltaI = (sum(actions(g,1:t-1) == 2) - sum(actions(g,1:t-1) == 1)) * baseline_info_bonus * T;
                               
                
                % probability of choosing bandit 1
                p = 1 / (1 + exp(-(deltaR+deltaI+side_bias)/(noise(t))));
                
                if sim
                    % simulate behavior
                    u = rand(1,1);
                    if u <= p
                        actions(g,t) = 1;
                        rewards(g,t) = mdp.bandit1_schedule(g,t);
                    else
                        actions(g,t) = 2;
                        rewards(g,t) = mdp.bandit2_schedule(g,t);
                    end
                end
                
                action_probs(g,t) = mod(actions(g,t),2)*p + (1-mod(actions(g,t),2))*(1-p);
            end
                
            
            % left bandit choice so mu1 updates
            if (actions(g,t) == 1) 
                exp_vals(g,t) = mu1(t);
                pred_errors(g,t) = (reward_sensitivity*rewards(g,t)) - exp_vals(g,t);
                alpha(g,t) = learning_rate * associability1(g,t);
                pred_errors_alpha(g,t) = alpha(t) * pred_errors(g,t);
                mu1(t+1) = mu1(t) + pred_errors_alpha(g,t);
                mu2(t+1) = mu2(t); 
                
                associability1(g,t+1) = (1 - associability_weight)*associability1(g,t) + associability_weight*abs(pred_errors(g,t));
                associability2(g,t+1) = associability2(g,t);
                
            else % right bandit choice so mu2 updates
                exp_vals(g,t) = mu2(t);
                pred_errors(g,t) = (reward_sensitivity*rewards(g,t)) - exp_vals(g,t);
                alpha(g,t) = learning_rate * associability1(g,t);
                pred_errors_alpha(g,t) = alpha(t) * pred_errors(g,t);
                mu2(t+1) = mu2(t) + pred_errors_alpha(g,t);
                mu1(t+1) = mu1(t); 
                
                associability1(g,t+1) = associability1(g,t);
                associability1(g,t+1) = (1 - associability_weight)*associability1(g,t) + associability_weight*abs(pred_errors(g,t));

            end

        end
    end

    
    
    model_output.action_probs = action_probs;
    model_output.exp_vals = exp_vals;
    model_output.pred_errors = pred_errors;
    model_output.pred_errors_alpha = pred_errors_alpha;
    model_output.alpha = alpha;
    model_output.actions = actions;
    model_output.rewards = rewards;

end